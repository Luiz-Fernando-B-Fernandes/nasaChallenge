1 - Qual o objetivo do comando cache em Spark?
R - A função 'cache' é utilizada para armazenar em memória partes intermediárias de dados (RDD) em memoria, que é limitada pela memória da instancia local

2 - O mesmo código implementado em Spark é normalmente mais rápido que a implementação equivalente em MapReduce. Por quê?
R - Utilizando o Spark é possível que os dados sejam lidos diretamente do cluster utilizando sua própria memória , ja que ele le dados direto do Hadoop Distributed File System, o 'HDFS'

3 - Qual​ ​ é ​ ​ a ​ ​ função​ ​ do​ ​ SparkContext​ ?
R - Ele é basicamente o cliente de execução do spark, com ele é possivel definir e criar RDDs, rodar 'jobs' entre outros, além de parametrizar tais serviços 


4 - Explique​ ​ com​ ​ suas​ ​ palavras​ ​ ​ o ​ ​ que​ ​ é ​ ​ Resilient​ ​ Distributed​ ​ Datasets​​ ​ (RDD).
R - O RDD é uma forma de armazenamento instanciando variações de dados em multiplos nodes particionando. Eles podem ser criados criando primeiramente um arquivo em um sistema de arquivos hadoop (ou outro sistema que o suporte)

5 - GroupByKey​ ​ é ​ ​ menos​ ​ eficiente​ ​ que​ ​ reduceByKey​ ​ em​ ​ grandes​ ​ dataset.​ ​ Por​ ​ quê?
R - Isto acontece por que o GroupByKey gera dados em pares no nivel chave, valor, porém uma chave pode ter uma lista grande dados e que se por sua vez nao couber na memoria, sera divida gerando dados desnecessários em cada uma dessas operações
enquando o recudeByKey pode ser utilizando passando mais um argumento para 'redividir' as listas em listas ainda melhores, reduzindo assim ainda mais os dados e tornando mais dinamico o armazenamento quando chamado um dataset(Chave , Valor)

6 - Explique​ ​ o ​ ​ que​ ​ o ​ ​ código​ ​ Scala​ ​ abaixo​ ​ faz.
R - Primeiramente cria um arquivo de Texto chamado 'textFile' que recebe um arquivo do hadop 'hdfs'
  - Cria uma variavel que mapeia as linhas separando elas por espaço
  - Por ultimo pega as os valores de cara 'palavra' e os soma (utilizando _ + _) que seria a operação de soma entre 2 elementos
  - Por ultimo o codigo salva o 'counts' no mesmo arquivo de dados (hadoop)